{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** CNTK 210: Part A - COCO Data Loader **  \n",
    "In this tutorial, we will download the COCO Image dataset to be used for image captioning.\n",
    "\n",
    "The CNTK 210 tutorial is divided into 2 parts:  \n",
    "Part A: downloads the images and annotations and then builds CNTK .map and .ctf reader files.\n",
    "Part B: builds a Sequence-to-sequence model that we then train and test, using the downloaded images and annotations (captions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this tutorial relatively fast running, we only use a small subset of the COCO dataset - the 2017 validation set, which consists of 5000 images and 24,000 annotations (captions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading zip file from http://images.cocodataset.org/zips/val2017.zip ...\n",
      "unzipping files from images.zip ...\n",
      "unzip completed\n",
      "downloading zip file from http://images.cocodataset.org/annotations/annotations_trainval2017.zip ...\n",
      "unzipping files from annotations.zip ...\n",
      "unzip completed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "def unzip_files(url, to_dir, fn_to_test, zipfn):\n",
    "    \n",
    "    if (not os.path.exists(fn_to_test)):\n",
    "\n",
    "        if (not os.path.exists(zipfn)):\n",
    "            print(\"downloading zip file from\", url + \" ...\")\n",
    "            urllib.request.urlretrieve(url, zipfn)\n",
    "\n",
    "        print(\"unzipping files from\", zipfn + \" ...\")\n",
    "    \n",
    "        if (not os.path.exists(to_dir)):\n",
    "            os.mkdir(to_dir)\n",
    "            \n",
    "        with zipfile.ZipFile(zipfn, \"r\") as zipper:\n",
    "            zipper.extractall(to_dir)\n",
    "        \n",
    "        os.remove(zipfn)\n",
    "        print(\"unzip completed\")\n",
    "        \n",
    "# download & unzip image files if not present (into \"images\" subdir)\n",
    "unzip_files(\"http://images.cocodataset.org/zips/val2017.zip\", \"images\", \"images\\\\val2017\\\\000000000139.jpg\", \"images.zip\")\n",
    "\n",
    "# download & unzip annotation files if not present (into \"annotations\" subdir)\n",
    "unzip_files(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\", \"annotations\", \n",
    "            \"annotations\\\\annotations\\\\captions_val2017.json\", \"annotations.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annotation data...\n",
      "len(images)= 5000 len(annotations)= 25014\n",
      "len(captions)= 261108 len(ucaptions)= 8222\n",
      "annotation processing completed\n"
     ]
    }
   ],
   "source": [
    "def process_annotations():\n",
    "    \n",
    "    with open('annotations\\\\annotations\\\\captions_val2017.json') as json_text:\n",
    "        d = json.load(json_text)\n",
    "\n",
    "    print(\"Processing annotation data...\")\n",
    "    \n",
    "    images = d[\"images\"]\n",
    "    annotations = d[\"annotations\"]\n",
    "\n",
    "    # we need to access image data by the image id\n",
    "    image_dict = {}\n",
    "    for i in images:\n",
    "        id = i[\"id\"]\n",
    "        image_dict[id] = i[\"file_name\"]\n",
    "\n",
    "    print(\"len(images)=\", len(images), \"len(annotations)=\", len(annotations))\n",
    "    entries = []\n",
    "    captions = []\n",
    "\n",
    "    for a in annotations:\n",
    "        strcap = a[\"caption\"].lower().strip()\n",
    "        before = strcap\n",
    "\n",
    "        if (strcap.endswith(\".\") and not strcap.endswith(\"..\")):\n",
    "            strcap = strcap[:-1]\n",
    "\n",
    "        #if (\".\" in strcap):\n",
    "        #    print(\"before strcap=\", before, \"found_period=\", found_period)\n",
    "\n",
    "        caption = strcap.split()\n",
    "        captions = captions + caption\n",
    "\n",
    "        last_word = caption[-1]\n",
    "        fn = image_dict[a[\"image_id\"]]\n",
    "\n",
    "        entry = {\"imageid\": a[\"image_id\"], \"label\": last_word, \"fn\": fn, \"caption\": caption}\n",
    "        entries.append(entry)\n",
    "\n",
    "    # build dictionary of all words used in captions\n",
    "    captions = captions + [\"<s>\", \"</s>\"]   # add strings for sequence start and sequence end\n",
    "    ucaptions = set(captions)\n",
    "    udict = {}\n",
    "    next = 0\n",
    "    for u in ucaptions:\n",
    "        udict[u] = next\n",
    "        next += 1\n",
    "\n",
    "    print(\"len(captions)=\", len(captions), \"len(ucaptions)=\", len(ucaptions))\n",
    "\n",
    "    # shuffle entries so we can split into train and test\n",
    "    import random\n",
    "    random.shuffle(entries)\n",
    "\n",
    "    # split the data: 80% for training, and 20% for test\n",
    "    train_count = int(.8 * len(entries))\n",
    "    train = entries[:train_count]\n",
    "    test = entries[train_count:]\n",
    "    \n",
    "    print(\"annotation processing completed\")\n",
    "    return [train, test, udict]\n",
    "\n",
    "# read the annotations file and build the \"train\" and \"test\" data, along with a \"udict\" dictionary\n",
    "[train, test, udict] = process_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222 dict entries written to fn= data\\mc.dict\n",
      "20011 lines written to fn= data\\mc_train.map\n",
      "20011 lines written to fn= data\\mc_train.ctf\n",
      "5003 lines written to fn= data\\mc_test.map\n",
      "5003 lines written to fn= data\\mc_test.ctf\n",
      "Data loading tutorial completed.\n"
     ]
    }
   ],
   "source": [
    "def write_dict(data, dir, fn):\n",
    "    import pickle\n",
    "    \n",
    "    if (not os.path.exists(dir)):\n",
    "        os.mkdir(dir)\n",
    "    fn = dir + \"\\\\\" + fn\n",
    "    \n",
    "    with open(fn, \"wb\") as myfile:\n",
    "        pickle.dump(data, myfile)\n",
    "\n",
    "        print(len(data), \"dict entries written to fn=\", fn)\n",
    "        \n",
    "def write_files(data, udict, dir, fnbase):\n",
    "    \n",
    "    if (not os.path.exists(dir)):\n",
    "        os.mkdir(dir)\n",
    "\n",
    "    # write MAP file\n",
    "    fn = dir + \"\\\\\" + fnbase + \".map\"\n",
    "    with open(fn, \"w\") as text_file:\n",
    "        seqnum = 0\n",
    "        for e in data:\n",
    "            label_id = udict[e[\"label\"]]\n",
    "            line = \"images\\\\\" + e[\"fn\"] + \"\\t\" + str(label_id) + \"\\t\" + str(seqnum) + \"\\r\\n\"\n",
    "            text_file.write(line)\n",
    "            seqnum += 1\n",
    "\n",
    "        print(seqnum, \"lines written to fn=\", fn)\n",
    "        \n",
    "    # write CTF file\n",
    "    fn = dir + \"\\\\\" + fnbase + \".ctf\"\n",
    "    with open(fn, \"w\") as text_file:\n",
    "        seqnum = 0\n",
    "        for e in data:\n",
    "            caption = e[\"caption\"]\n",
    "            caption = [\"<s>\"] + caption + [\"</s>\"]\n",
    "            for c in caption:\n",
    "                word_id = udict[c]\n",
    "                line = str(seqnum) + \"\\t| label \" + str(word_id) + \":1\\t| # \" + c + \"\\r\\n\"\n",
    "                text_file.write(line)\n",
    "            seqnum += 1\n",
    "\n",
    "        print(seqnum, \"lines written to fn=\", fn)\n",
    "        \n",
    "# write the dictionary file (to the \"data\" subdir)\n",
    "write_dict(udict, \"data\", \"mc.dict\")\n",
    "\n",
    "# write the train and test files (to the \"data\" subdir)\n",
    "write_files(train, udict, \"data\", \"mc_train\")\n",
    "write_files(test, udict, \"data\", \"mc_test\")\n",
    "\n",
    "print(\"Data loading tutorial completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
